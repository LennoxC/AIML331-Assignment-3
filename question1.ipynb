{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# AIML331 Assignment 3\n",
    "### crowelenn, 300607096\n",
    "\n",
    "In this assignment, we focus on using modern machine learning techniques to classify animals from the OxfordIIITPet dataset. The animals are to be classified into the following categories:\n",
    "\n",
    "- long-haired cat\n",
    "- short-haired cat\n",
    "- long-haired dog\n",
    "- short-haired dog\n",
    "\n",
    "In the first part of the assignment, we will train and evaluate a convolutional neural network to perform this task."
   ],
   "id": "bfc6c8d4e96e1e19"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Setup\n",
    "Import libraries, and load the dataset. The function to load the dataset has been provided in `dataset_wrapper` and the data is stored in the `./data` local directory."
   ],
   "id": "2d928d4a05cd3671"
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-23T05:03:13.241857Z",
     "start_time": "2025-05-23T05:03:09.231288Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from dataset_wrapper import get_pet_datasets\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T05:03:45.392079Z",
     "start_time": "2025-05-23T05:03:13.241857Z"
    }
   },
   "cell_type": "code",
   "source": "train_dataset, val_dataset, test_dataset = get_pet_datasets(img_width=128, img_height=128,root_path='./data' )",
   "id": "123698bd9c4d206",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T05:03:45.648153Z",
     "start_time": "2025-05-23T05:03:45.642705Z"
    }
   },
   "cell_type": "code",
   "source": "print(f\"Loaded data, train = {len(train_dataset)}, test = {len(test_dataset)}\")",
   "id": "35997bf2379270b7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data, train = 5719, test = 716\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T05:03:45.719400Z",
     "start_time": "2025-05-23T05:03:45.667527Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# check that cuda is working\n",
    "torch.cuda.is_available()"
   ],
   "id": "8293db5e150cb795",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Dataloaders, training devices etc\n",
    "All the pytorch stuff that isn't the neural network itself"
   ],
   "id": "1d9d18114fd4a07c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T05:03:45.734393Z",
     "start_time": "2025-05-23T05:03:45.725595Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# define the compute device\n",
    "compute_device = torch.device('cuda:0')"
   ],
   "id": "f27c9fd69e39b810",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T05:03:45.769261Z",
     "start_time": "2025-05-23T05:03:45.762717Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# load the datasets\n",
    "batch_size = 64\n",
    "training_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "validation_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "testing_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ],
   "id": "6fa72c20ee844705",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Defining the neural network\n",
    "\n",
    "**Task 1**:  Build a CNN architecture from scratch using PyTorch. You may utilise basic layers and components provided by PyTorch (e.g., Conv2D, BatchNorm2D, MaxPool2D, Linear, ReLU), but you are required to design and assemble the overall model architecture (including the loss function) independently"
   ],
   "id": "6f3cd2d21205bb73"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T05:03:45.814173Z",
     "start_time": "2025-05-23T05:03:45.807340Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class PetsConvNet(nn.Module):\n",
    "    def __init__(self, num_classes=4):\n",
    "        super(PetsConvNet, self).__init__()\n",
    "\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3,\n",
    "                      out_channels=16,\n",
    "                      kernel_size=5,\n",
    "                      stride=1,\n",
    "                      padding=2),\n",
    "            nn.BatchNorm2d(num_features=16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        # l1 output dim = 128 - 5 + 1 + 2*2 / 2\n",
    "        #               = 64\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=16,\n",
    "                      out_channels=32,\n",
    "                      kernel_size=5,\n",
    "                      stride=1,\n",
    "                      padding=2),\n",
    "            nn.BatchNorm2d(num_features=32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        # l2 output dim = 64 - 5 + 1 + 2*2 / 2\n",
    "        #               = 32\n",
    "\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=32,\n",
    "                      out_channels=64,\n",
    "                      kernel_size=5,\n",
    "                      stride=1,\n",
    "                      padding=2),\n",
    "            nn.BatchNorm2d(num_features=64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        # l3 output dim = 32 - 5 + 1 + 2*2 / 2\n",
    "        #               = 16\n",
    "\n",
    "        self.fc = nn.Linear(16*16*64, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ],
   "id": "3d9f16d5e1d6a198",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Functions for training & testing",
   "id": "c85c6aacd63b0e62"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T05:03:45.835544Z",
     "start_time": "2025-05-23T05:03:45.825198Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def do_training(model, experiment_name, criterion, optimizer, num_epochs=20, patience=5):\n",
    "    writer = SummaryWriter('runs/'+experiment_name)\n",
    "\n",
    "    min_validation_loss = None\n",
    "    best_model_state = None # store the best model here. Re-instate this if early stopping is triggered\n",
    "    wait = 0\n",
    "\n",
    "    # make sure the model is starting with new weights\n",
    "    for layer in model.children():\n",
    "        if hasattr(layer, 'reset_parameters'):\n",
    "            layer.reset_parameters()\n",
    "\n",
    "    steps = len(training_dataloader)\n",
    "    for epoch in range(num_epochs): # epoch iteration loop\n",
    "        model.train()\n",
    "        train_loss_epoch_total = 0\n",
    "        batches_count = 0\n",
    "\n",
    "        for i, (images, labels) in enumerate(training_dataloader):\n",
    "\n",
    "            if i == 0:\n",
    "                writer.add_graph(model, images.to(compute_device))\n",
    "            images = images.to(compute_device)\n",
    "            labels = labels.to(compute_device)\n",
    "\n",
    "            # forwards\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # backpropogation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss_epoch_total += loss.item()\n",
    "            batches_count += 1\n",
    "\n",
    "        train_loss = train_loss_epoch_total / batches_count\n",
    "        writer.add_scalar('Loss/train', train_loss, epoch+1)\n",
    "\n",
    "        # validation accuracy\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            val_loss_epoch_total = 0\n",
    "            val_batches_count = 0\n",
    "\n",
    "            for images, labels in validation_dataloader:\n",
    "                images = images.to(compute_device)\n",
    "                labels = labels.to(compute_device)\n",
    "                outputs = model(images)\n",
    "                val_loss = criterion(outputs, labels)\n",
    "\n",
    "                val_loss_epoch_total += val_loss.item()\n",
    "                val_batches_count += 1\n",
    "\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        val_acc = 100 * correct / total\n",
    "        val_loss = val_loss_epoch_total / val_batches_count\n",
    "\n",
    "        writer.add_scalar('Accuracy/validation', val_acc, epoch)\n",
    "        writer.add_scalar('Loss/validation', val_loss, epoch)\n",
    "\n",
    "        if min_validation_loss is None or val_loss < min_validation_loss:\n",
    "            min_validation_loss = val_loss\n",
    "            best_model_state = model.state_dict() # save the best weights\n",
    "            wait = 0\n",
    "        else:\n",
    "            wait += 1\n",
    "\n",
    "        if wait >= patience:\n",
    "            break # exit early if there has been no improvement in validation loss\n",
    "\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "        print(\"Best model weights restored.\")\n",
    "\n",
    "    writer.close()\n",
    "\n",
    "    return model"
   ],
   "id": "9169878aba104c0d",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T05:03:45.851517Z",
     "start_time": "2025-05-23T05:03:45.846579Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def do_testing(model, dataloader):\n",
    "    model.eval()  # eval mode (batchnorm uses moving mean/variance instead of mini-batch mean/variance)\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in dataloader:\n",
    "            images = images.to(compute_device)\n",
    "            labels = labels.to(compute_device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            accuracy = 100 * correct / total\n",
    "\n",
    "        return accuracy, 'Accuracy of the model on the provided images: {} %'.format(accuracy)\n"
   ],
   "id": "8e63274732ef50a2",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Task 2** Train the model for a few epochs until the loss value stabilises or flattens, indicating convergence (normally within 20 epochs for this dataset)",
   "id": "ceae555c85a4ae58"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T05:03:45.988160Z",
     "start_time": "2025-05-23T05:03:45.860129Z"
    }
   },
   "cell_type": "code",
   "source": [
    "baseline_model = PetsConvNet(num_classes=4).to(compute_device)\n",
    "weights = [600/7149, 1771/7149, 2590/7149, 2188/7149]\n",
    "class_weights = torch.FloatTensor(weights).cuda()\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = torch.optim.Adam(baseline_model.parameters(), lr=0.01)"
   ],
   "id": "b5f818d7a2ff305f",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T05:03:46.427643Z",
     "start_time": "2025-05-23T05:03:46.421826Z"
    }
   },
   "cell_type": "code",
   "source": [
    "baseline_num_params = sum(param.numel() for param in baseline_model.parameters())\n",
    "print(f\"Parameters count = {baseline_num_params}\")"
   ],
   "id": "9f05a7c29efc7466",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters count = 131076\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T01:40:48.136054Z",
     "start_time": "2025-05-16T01:37:14.775888Z"
    }
   },
   "cell_type": "code",
   "source": "baseline_model_trained = do_training(baseline_model, experiment_name='cnn_baseline', criterion=criterion, optimizer=optimizer, num_epochs=50, patience=10)",
   "id": "7e08c03a5ac84eb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model weights restored.\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T01:40:49.156980Z",
     "start_time": "2025-05-16T01:40:48.266277Z"
    }
   },
   "cell_type": "code",
   "source": "acc, acc_string = do_testing(baseline_model_trained, validation_dataloader)",
   "id": "ef5a35aecf3c2509",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T01:40:49.186359Z",
     "start_time": "2025-05-16T01:40:49.175006Z"
    }
   },
   "cell_type": "code",
   "source": "print(acc_string)",
   "id": "3043e522c7b875f5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the provided images: 46.9187675070028 %\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The baseline model achieves a test accuracy of 49.72%. This is good considering training the model from scratch on only ~5000 images.\n",
    "\n",
    "### Experiments\n",
    "In the `cnn_models` file I have defined the following classes:\n",
    "\n",
    "- PetsConvNetBaseline - this was defined in the question1.ipynb\n",
    "- PetsConvNetNoBatchNorm - baseline without batch normalization\n",
    "- PetsConvNetLeakyRelu - baseline with a leakyReLU activation function\n",
    "- PetsConvNetGelu - baseline with a GELU activation function\n",
    "- PetsConvNetL5 - baseline with 5 layers\n",
    "- PetsConvNetL7 - baseline with 7 layers\n",
    "\n",
    "Each of these will be trained & evaluated with the functions above to fairly compare their performance."
   ],
   "id": "be5a98f15b0995a0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T05:16:07.451990Z",
     "start_time": "2025-05-23T05:16:07.429588Z"
    }
   },
   "cell_type": "code",
   "source": "from cnn_models import PetsConvNetNoBatchNorm, PetsConvNetLeakyRelu, PetsConvNetGelu, PetsConvNetL5, PetsConvNetL7",
   "id": "44666081fe7f6a08",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T01:40:49.285059Z",
     "start_time": "2025-05-16T01:40:49.263349Z"
    }
   },
   "cell_type": "code",
   "source": [
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "learningrate = 0.01\n",
    "epochs = 50\n",
    "patience = 10"
   ],
   "id": "cf293cafed1e8a5a",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Define the Models",
   "id": "dc072952033b197a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T05:16:10.108600Z",
     "start_time": "2025-05-23T05:16:10.055820Z"
    }
   },
   "cell_type": "code",
   "source": [
    "nobatchnorm_model = PetsConvNetNoBatchNorm(num_classes=4).to(compute_device)\n",
    "\n",
    "leakyrelu_model = PetsConvNetLeakyRelu(num_classes=4).to(compute_device)\n",
    "gelu_model = PetsConvNetGelu(num_classes=4).to(compute_device)\n",
    "\n",
    "L5_model = PetsConvNetL5(num_classes=4).to(compute_device)\n",
    "L7_model = PetsConvNetL7(num_classes=4).to(compute_device)"
   ],
   "id": "a797e1dc7cdc9ed0",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T05:18:54.383452Z",
     "start_time": "2025-05-23T05:18:54.367893Z"
    }
   },
   "cell_type": "code",
   "source": [
    "models_params = [baseline_model, nobatchnorm_model, leakyrelu_model, gelu_model, L5_model, L7_model]\n",
    "model_param_counts = []\n",
    "\n",
    "for i in range(len(models_params)):\n",
    "    model_param_counts.append(sum(param.numel() for param in models_params[i].parameters()))"
   ],
   "id": "bc0c127eee8e9c02",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T05:18:57.075492Z",
     "start_time": "2025-05-23T05:18:56.970520Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(model_param_counts)"
   ],
   "id": "564c4351d92bac72",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([4., 1., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       " array([130852., 181428., 232004., 282580., 333156., 383732., 434308.,\n",
       "        484884., 535460., 586036., 636612.]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### No Batch Normalization",
   "id": "9452afa8e4ae85f4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T01:44:11.737923Z",
     "start_time": "2025-05-16T01:40:49.321203Z"
    }
   },
   "cell_type": "code",
   "source": [
    "optimizer = torch.optim.Adam(nobatchnorm_model.parameters(), lr=learningrate)\n",
    "\n",
    "nobatchnorm_model_trained = do_training(nobatchnorm_model, experiment_name='cnn_no_batchnorm', criterion=criterion, optimizer=optimizer, num_epochs=epochs, patience=patience)\n",
    "acc, acc_string = do_testing(nobatchnorm_model_trained, validation_dataloader)\n",
    "print(acc_string)"
   ],
   "id": "39087fe02761e1c1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model weights restored.\n",
      "Accuracy of the model on the provided images: 34.593837535014 %\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Leaky ReLU",
   "id": "36d276cb9fda814e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T01:47:42.909258Z",
     "start_time": "2025-05-16T01:44:11.808654Z"
    }
   },
   "cell_type": "code",
   "source": [
    "optimizer = torch.optim.Adam(leakyrelu_model.parameters(), lr=learningrate)\n",
    "\n",
    "leakyrelu_model_trained = do_training(leakyrelu_model, experiment_name='cnn_leaky_relu', criterion=criterion, optimizer=optimizer, num_epochs=epochs, patience=patience)\n",
    "acc, acc_string = do_testing(leakyrelu_model_trained, validation_dataloader)\n",
    "print(acc_string)"
   ],
   "id": "6d8ef4e295b4c17a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model weights restored.\n",
      "Accuracy of the model on the provided images: 45.65826330532213 %\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### GELU",
   "id": "bcdf2c3c2652d2dd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T01:51:27.760728Z",
     "start_time": "2025-05-16T01:47:42.954586Z"
    }
   },
   "cell_type": "code",
   "source": [
    "optimizer = torch.optim.Adam(gelu_model.parameters(), lr=learningrate)\n",
    "\n",
    "gelu_model_trained = do_training(gelu_model, experiment_name='cnn_gelu', criterion=criterion, optimizer=optimizer, num_epochs=epochs, patience=patience)\n",
    "acc, acc_string = do_testing(gelu_model_trained, validation_dataloader)\n",
    "print(acc_string)"
   ],
   "id": "6f0b1c4f6ab8dba7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model weights restored.\n",
      "Accuracy of the model on the provided images: 47.05882352941177 %\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 5 Layers",
   "id": "259b1339f600cce9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T01:58:00.703223Z",
     "start_time": "2025-05-16T01:51:27.815811Z"
    }
   },
   "cell_type": "code",
   "source": [
    "optimizer = torch.optim.Adam(L5_model.parameters(), lr=learningrate)\n",
    "\n",
    "L5_model_trained = do_training(L5_model, experiment_name='cnn_L5', criterion=criterion, optimizer=optimizer, num_epochs=epochs, patience=patience)\n",
    "acc, acc_string = do_testing(L5_model_trained, validation_dataloader)\n",
    "print(acc_string)"
   ],
   "id": "86240e753d7c90fc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model weights restored.\n",
      "Accuracy of the model on the provided images: 51.680672268907564 %\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 7 Layers",
   "id": "4fff74062dd7a77a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T02:06:07.265756Z",
     "start_time": "2025-05-16T01:58:00.756197Z"
    }
   },
   "cell_type": "code",
   "source": [
    "optimizer = torch.optim.Adam(L7_model.parameters(), lr=learningrate)\n",
    "\n",
    "L7_model_trained = do_training(L7_model, experiment_name='cnn_L7', criterion=criterion, optimizer=optimizer, num_epochs=epochs, patience=patience)\n",
    "acc, acc_string = do_testing(L7_model_trained, validation_dataloader)\n",
    "print(acc_string)"
   ],
   "id": "8ab2f1989b9de1eb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model weights restored.\n",
      "Accuracy of the model on the provided images: 56.72268907563025 %\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Residual Integration\n",
    "Modify your CNN architecture to incorporate a residual connection around the CNN layer. Specifically, instead of computing the output as CNN(x), the modified network should compute the output as x + CNN(x), enabling the model to learn residual mappings.\n",
    "\n",
    "I decided it would make most sense to re-build the 7-layer CNN with skip connections, as this has the most parameters and is most likely to overfit."
   ],
   "id": "461907d236be4466"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T05:03:54.619419Z",
     "start_time": "2025-05-16T05:03:54.612406Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# the network with skip connections is going to be made up of these\n",
    "class ResNetBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, downsample=False):\n",
    "        super(ResNetBlock, self).__init__()\n",
    "\n",
    "        pooling_factor = 2 if downsample else 1 # downsample by a factor of 2\n",
    "        padding = (kernel_size - 1) // 2\n",
    "\n",
    "        self.convolution1 = nn.Conv2d(in_channels=in_channels,\n",
    "                                      out_channels=out_channels,\n",
    "                                      kernel_size=kernel_size,\n",
    "                                      stride=pooling_factor,\n",
    "                                      padding=padding,\n",
    "                                      bias=False)\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.BatchNorm = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.downsample_skip = None\n",
    "\n",
    "        # if we are downsampling or changing the number of channels,\n",
    "        #   then we will need to apply this to the x 'skip connection'\n",
    "        #   input too, otherwise the piecewise addition will fail\n",
    "        if downsample or in_channels != out_channels:\n",
    "            self.downsample_skip = nn.Sequential(\n",
    "                nn.Conv2d(in_channels=in_channels,\n",
    "                          out_channels=out_channels,\n",
    "                          kernel_size=1,\n",
    "                          stride=pooling_factor,\n",
    "                          bias=False), # BatchNorm has a bias so this is redundant\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "        self.BatchNorm2 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        input = x # save the original input here\n",
    "\n",
    "        # push the input through the 'regular' CNN components\n",
    "        out = self.BatchNorm(self.relu(self.convolution1(x)))\n",
    "        #out = self.relu(self.BatchNorm(self.convolution1(x)))\n",
    "\n",
    "        # I could add more batch normalization?\n",
    "\n",
    "        # if downsampling, then this will be defined. The input needs to be transformed\n",
    "        if self.downsample_skip is not None:\n",
    "            input = self.downsample_skip(x)\n",
    "\n",
    "        # piecewise addition for the skip connection\n",
    "        out += input\n",
    "\n",
    "        # experimental\n",
    "        out = self.relu2(self.BatchNorm2(out))\n",
    "\n",
    "        return out\n"
   ],
   "id": "cb24adf6df0a7d68",
   "outputs": [],
   "execution_count": 61
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T04:47:01.926955Z",
     "start_time": "2025-05-16T04:47:01.910647Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class PetsConvNetSkipConnectionL3(nn.Module):\n",
    "    def __init__(self, num_classes=4):\n",
    "        super(PetsConvNetSkipConnectionL3, self).__init__()\n",
    "\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3,\n",
    "                      out_channels=16,\n",
    "                      kernel_size=5,\n",
    "                      stride=1,\n",
    "                      padding=2),\n",
    "            nn.BatchNorm2d(num_features=16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        # l1 output dim = 128 - 5 + 1 + 2*2 / 2\n",
    "        #               = 64\n",
    "        self.layer2 = ResNetBlock(16, 32, kernel_size=5, downsample=True)\n",
    "        # l2 output dim = 32\n",
    "\n",
    "        self.layer3 = ResNetBlock(32, 64, kernel_size=5, downsample=True)\n",
    "        # l3 output dim = 16\n",
    "\n",
    "        self.fc = nn.Linear(16*16*64, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ],
   "id": "89872e78b247c080",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T05:03:57.408783Z",
     "start_time": "2025-05-16T05:03:57.401782Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class PetsConvNetSkipConnectionL7(nn.Module):\n",
    "    def __init__(self, num_classes=4):\n",
    "        super(PetsConvNetSkipConnectionL7, self).__init__()\n",
    "\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3,\n",
    "                      out_channels=16,\n",
    "                      kernel_size=5,\n",
    "                      stride=1,\n",
    "                      padding=2),\n",
    "            nn.BatchNorm2d(num_features=16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        # l1 output dim = 128 - 5 + 1 + 2*2 / 2\n",
    "        #               = 64\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=16,\n",
    "                      out_channels=32,\n",
    "                      kernel_size=5,\n",
    "                      stride=1,\n",
    "                      padding=2),\n",
    "            nn.BatchNorm2d(num_features=32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        # l2 output dim = 64 - 5 + 1 + 2*2 / 2\n",
    "        #               = 32\n",
    "\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=32,\n",
    "                      out_channels=64,\n",
    "                      kernel_size=5,\n",
    "                      stride=1,\n",
    "                      padding=2),\n",
    "            nn.BatchNorm2d(num_features=64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        # l3 output dim = 32 - 5 + 1 + 2*2 / 2\n",
    "        #               = 16\n",
    "\n",
    "        # push through layers 4 & 5 before pooling\n",
    "        # keep channel at 64 until layer 5\n",
    "        # use 3x3 convolutions as the inputs are only 16x16 now\n",
    "\n",
    "        self.layer4 = ResNetBlock(64, 64, kernel_size=3, downsample=False)\n",
    "        # l4 output dim = 16\n",
    "\n",
    "        self.layer5 = ResNetBlock(64, 128, kernel_size=3, downsample=True)\n",
    "        # l5 output dim = 8\n",
    "\n",
    "        self.layer6 = ResNetBlock(128, 128, kernel_size=3, downsample=False)\n",
    "        # l6 output dim = 8\n",
    "\n",
    "        self.layer7 = ResNetBlock(128, 256, kernel_size=3, downsample=True)\n",
    "        # l7 output dim = 4\n",
    "\n",
    "        self.fc = nn.Linear(4 * 4 * 256, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = self.layer5(out)\n",
    "        out = self.layer6(out)\n",
    "        out = self.layer7(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ],
   "id": "75ad5c9ba663fa06",
   "outputs": [],
   "execution_count": 62
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T04:48:40.732057Z",
     "start_time": "2025-05-16T04:47:32.950454Z"
    }
   },
   "cell_type": "code",
   "source": [
    "L3_skipconnection_model = PetsConvNetSkipConnectionL3(num_classes=4).to(compute_device)\n",
    "optimizer = torch.optim.Adam(L3_skipconnection_model.parameters(), lr=learningrate)\n",
    "\n",
    "L3_skipconnection_model_trained = do_training(L3_skipconnection_model, experiment_name='cnn_skip_L3', criterion=criterion, optimizer=optimizer, num_epochs=epochs)\n",
    "acc, acc_string = do_testing(L3_skipconnection_model_trained, validation_dataloader)\n",
    "print(acc_string)"
   ],
   "id": "3e65925e8363516a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model weights restored.\n",
      "Accuracy of the model on the provided images: 34.173669467787114 %\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T05:12:18.661096Z",
     "start_time": "2025-05-16T05:08:39.337943Z"
    }
   },
   "cell_type": "code",
   "source": [
    "L7_skipconnection_model = PetsConvNetSkipConnectionL7(num_classes=4).to(compute_device)\n",
    "optimizer = torch.optim.Adam(L7_skipconnection_model.parameters(), lr=learningrate)\n",
    "\n",
    "L7_skipconnection_model_trained = do_training(L7_skipconnection_model, experiment_name='cnn_skip_L7', criterion=criterion, optimizer=optimizer, num_epochs=50, patience=10)\n",
    "acc, acc_string = do_testing(L7_skipconnection_model_trained, validation_dataloader)\n",
    "print(acc_string)"
   ],
   "id": "18f4761ad4697e7a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model weights restored.\n",
      "Accuracy of the model on the provided images: 43.13725490196079 %\n"
     ]
    }
   ],
   "execution_count": 65
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Final Model Testing\n",
    "Until now, the models have only been evaluated on the validation set. They should be evaluated on the Testing set to assess their generalisation performance."
   ],
   "id": "efa0acba2324bdd0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T02:12:51.747527Z",
     "start_time": "2025-05-16T02:12:50.602366Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Baseline Model\n",
    "print(do_testing(baseline_model_trained, testing_dataloader)[1])"
   ],
   "id": "7d37f643bae7c61a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the provided images: 49.16201117318436 %\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T02:12:52.587207Z",
     "start_time": "2025-05-16T02:12:51.803715Z"
    }
   },
   "cell_type": "code",
   "source": "print(do_testing(nobatchnorm_model_trained, testing_dataloader)[1])",
   "id": "4ce152d5a77ec635",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the provided images: 39.245810055865924 %\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T02:12:54.281939Z",
     "start_time": "2025-05-16T02:12:52.598699Z"
    }
   },
   "cell_type": "code",
   "source": "print(do_testing(leakyrelu_model_trained, testing_dataloader)[1])",
   "id": "c2b264958ead4526",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the provided images: 50.977653631284916 %\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T02:12:55.150960Z",
     "start_time": "2025-05-16T02:12:54.306794Z"
    }
   },
   "cell_type": "code",
   "source": "print(do_testing(gelu_model_trained, testing_dataloader)[1])",
   "id": "70c7b78adca9dc69",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the provided images: 47.90502793296089 %\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T02:12:56.081753Z",
     "start_time": "2025-05-16T02:12:55.191880Z"
    }
   },
   "cell_type": "code",
   "source": "print(do_testing(L5_model_trained, testing_dataloader)[1])",
   "id": "187329882c8105e6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the provided images: 52.23463687150838 %\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T02:12:57.006686Z",
     "start_time": "2025-05-16T02:12:56.089005Z"
    }
   },
   "cell_type": "code",
   "source": "print(do_testing(L7_model_trained, testing_dataloader)[1])",
   "id": "b8e86bb5f8eda8da",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the provided images: 56.28491620111732 %\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T02:12:57.910028Z",
     "start_time": "2025-05-16T02:12:57.033747Z"
    }
   },
   "cell_type": "code",
   "source": "print(do_testing(L3_skipconnection_model_trained, testing_dataloader)[1])",
   "id": "f52b45be1209e9c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the provided images: 43.99441340782123 %\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T02:12:58.872910Z",
     "start_time": "2025-05-16T02:12:57.930319Z"
    }
   },
   "cell_type": "code",
   "source": "print(do_testing(L7_skipconnection_model_trained, testing_dataloader)[1])",
   "id": "3ae40e6fc7d961ac",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the provided images: 49.16201117318436 %\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Testing the inference time\n",
    "I am training a new baseline model, as I have already written the bulk of the report, and don't want to have to update the baseline model accuracy & figures throughout."
   ],
   "id": "3667908b39429b52"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T06:18:33.873558Z",
     "start_time": "2025-05-23T06:14:30.227187Z"
    }
   },
   "cell_type": "code",
   "source": [
    "baseline_model_inference = PetsConvNet(num_classes=4).to(compute_device)\n",
    "weights = [600 / 7149, 1771 / 7149, 2590 / 7149, 2188 / 7149]\n",
    "class_weights = torch.FloatTensor(weights).cuda()\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = torch.optim.Adam(baseline_model_inference.parameters(), lr=0.01)\n",
    "\n",
    "baseline_model_inference = do_training(baseline_model_inference, experiment_name='cnn_baseline', criterion=criterion, optimizer=optimizer, num_epochs=50, patience=10)"
   ],
   "id": "a1a9b3cd76c43c0d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model weights restored.\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T05:39:14.774167Z",
     "start_time": "2025-05-23T05:39:13.431986Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import time\n",
    "\n",
    "baseline_model_inference.eval()\n",
    "\n",
    "total_time = 0.0\n",
    "total_images = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, _ in testing_dataloader:\n",
    "        inputs = inputs.to(compute_device)\n",
    "        batch_size = inputs.size(0)\n",
    "\n",
    "        start_time = time.time()\n",
    "        _ = baseline_model_inference(inputs)\n",
    "        end_time = time.time()\n",
    "\n",
    "        total_time += end_time - start_time\n",
    "        total_images += batch_size\n",
    "\n",
    "average_inference = total_time / total_images\n",
    "print(f\"Average image inference time: {average_inference}\")"
   ],
   "id": "8a3ae76e7a5210a4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average image inference time: 5.0445841677362025e-05\n"
     ]
    }
   ],
   "execution_count": 33
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
