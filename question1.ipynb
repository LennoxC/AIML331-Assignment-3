{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# AIML331 Assignment 3\n",
    "### crowelenn, 300607096\n",
    "\n",
    "In this assignment, we focus on using modern machine learning techniques to classify animals from the OxfordIIITPet dataset. The animals are to be classified into the following categories:\n",
    "\n",
    "- long-haired cat\n",
    "- short-haired cat\n",
    "- long-haired dog\n",
    "- short-haired dog\n",
    "\n",
    "In the first part of the assignment, we will train and evaluate a convolutional neural network to perform this task."
   ],
   "id": "bfc6c8d4e96e1e19"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Setup\n",
    "Import libraries, and load the dataset. The function to load the dataset has been provided in `dataset_wrapper` and the data is stored in the `./data` local directory."
   ],
   "id": "2d928d4a05cd3671"
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-14T01:02:25.053268Z",
     "start_time": "2025-05-14T01:02:20.757135Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 1,
   "source": [
    "from torch import nn\n",
    "\n",
    "from dataset_wrapper import get_pet_datasets"
   ],
   "id": "initial_id"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T02:31:47.676428Z",
     "start_time": "2025-05-14T02:31:16.094204Z"
    }
   },
   "cell_type": "code",
   "source": "train_dataset, val_dataset, test_dataset = get_pet_datasets(img_width=128, img_height=128,root_path='./data' )",
   "id": "123698bd9c4d206",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T02:32:07.549017Z",
     "start_time": "2025-05-14T02:32:07.540158Z"
    }
   },
   "cell_type": "code",
   "source": "print(f\"Loaded data, train = {len(train_dataset)}, test = {len(test_dataset)}\")",
   "id": "35997bf2379270b7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data, train = 5719, test = 716\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Import the required libraries:",
   "id": "11f07191455d7f0a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T02:40:49.166168Z",
     "start_time": "2025-05-14T02:40:49.160925Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn"
   ],
   "id": "526dec78bafdb960",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T02:19:02.160899Z",
     "start_time": "2025-05-14T02:19:02.148155Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# check that cuda is working\n",
    "torch.cuda.is_available()"
   ],
   "id": "8293db5e150cb795",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Dataloaders, training devices etc\n",
    "All the pytorch stuff that isn't the neural network itself"
   ],
   "id": "1d9d18114fd4a07c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T02:52:57.703264Z",
     "start_time": "2025-05-14T02:52:57.693120Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# define the compute device\n",
    "compute_device = torch.device('cuda:0')"
   ],
   "id": "f27c9fd69e39b810",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T02:54:47.371906Z",
     "start_time": "2025-05-14T02:54:47.364879Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# load the datasets\n",
    "batch_size = 64\n",
    "training_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "validation_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "testing_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ],
   "id": "6fa72c20ee844705",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Defining the neural network",
   "id": "6f3cd2d21205bb73"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T02:40:52.001476Z",
     "start_time": "2025-05-14T02:40:51.989489Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class PetsConvNet(nn.Module):\n",
    "    def __init__(self, num_classes=4):\n",
    "        super(PetsConvNet, self).__init__()\n",
    "\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3,\n",
    "                      out_channels=16,\n",
    "                      kernel_size=5,\n",
    "                      stride=1,\n",
    "                      padding=2),\n",
    "            nn.BatchNorm2d(num_features=16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        # l1 output dim = 128 - 5 + 1 + 2*2 / 2\n",
    "        #               = 64\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=16,\n",
    "                      out_channels=32,\n",
    "                      kernel_size=5,\n",
    "                      stride=1,\n",
    "                      padding=2),\n",
    "            nn.BatchNorm2d(num_features=32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        # l2 output dim = 64 - 5 + 1 + 2*2 / 2\n",
    "        #               = 32\n",
    "\n",
    "        # output = input - filter + 1 + 2*padding\n",
    "        self.fc = nn.Linear(32*32*32, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ],
   "id": "3d9f16d5e1d6a198",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Building the model and training",
   "id": "c85c6aacd63b0e62"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T02:55:03.065928Z",
     "start_time": "2025-05-14T02:55:03.050689Z"
    }
   },
   "cell_type": "code",
   "source": "model = PetsConvNet(num_classes=4).to(compute_device)",
   "id": "8f38ac0543a0592a",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T03:03:02.790545Z",
     "start_time": "2025-05-14T02:59:50.650349Z"
    }
   },
   "cell_type": "code",
   "source": [
    "learning_rate = 0.01\n",
    "num_epochs = 20\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "steps = len(training_dataloader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(training_dataloader):\n",
    "        images = images.to(compute_device)\n",
    "        labels = labels.to(compute_device)\n",
    "\n",
    "        # forwards\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # backpropogation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
    "                   .format(epoch+1, num_epochs, i+1, steps, loss.item()))"
   ],
   "id": "9169878aba104c0d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Step [1/90], Loss: 0.9950\n",
      "Epoch [1/20], Step [11/90], Loss: 1.2070\n",
      "Epoch [1/20], Step [21/90], Loss: 0.9515\n",
      "Epoch [1/20], Step [31/90], Loss: 1.0213\n",
      "Epoch [1/20], Step [41/90], Loss: 1.1345\n",
      "Epoch [1/20], Step [51/90], Loss: 1.1915\n",
      "Epoch [1/20], Step [61/90], Loss: 0.9353\n",
      "Epoch [1/20], Step [71/90], Loss: 1.1946\n",
      "Epoch [1/20], Step [81/90], Loss: 1.1094\n",
      "Epoch [2/20], Step [1/90], Loss: 0.9098\n",
      "Epoch [2/20], Step [11/90], Loss: 0.9702\n",
      "Epoch [2/20], Step [21/90], Loss: 0.9113\n",
      "Epoch [2/20], Step [31/90], Loss: 1.0206\n",
      "Epoch [2/20], Step [41/90], Loss: 0.9871\n",
      "Epoch [2/20], Step [51/90], Loss: 1.0686\n",
      "Epoch [2/20], Step [61/90], Loss: 1.0128\n",
      "Epoch [2/20], Step [71/90], Loss: 1.0074\n",
      "Epoch [2/20], Step [81/90], Loss: 1.0826\n",
      "Epoch [3/20], Step [1/90], Loss: 0.9048\n",
      "Epoch [3/20], Step [11/90], Loss: 0.8468\n",
      "Epoch [3/20], Step [21/90], Loss: 0.8756\n",
      "Epoch [3/20], Step [31/90], Loss: 1.0860\n",
      "Epoch [3/20], Step [41/90], Loss: 1.0895\n",
      "Epoch [3/20], Step [51/90], Loss: 1.2954\n",
      "Epoch [3/20], Step [61/90], Loss: 1.0952\n",
      "Epoch [3/20], Step [71/90], Loss: 1.1694\n",
      "Epoch [3/20], Step [81/90], Loss: 1.0443\n",
      "Epoch [4/20], Step [1/90], Loss: 0.7938\n",
      "Epoch [4/20], Step [11/90], Loss: 0.7049\n",
      "Epoch [4/20], Step [21/90], Loss: 1.0066\n",
      "Epoch [4/20], Step [31/90], Loss: 0.8314\n",
      "Epoch [4/20], Step [41/90], Loss: 1.0195\n",
      "Epoch [4/20], Step [51/90], Loss: 1.1056\n",
      "Epoch [4/20], Step [61/90], Loss: 1.1318\n",
      "Epoch [4/20], Step [71/90], Loss: 0.8922\n",
      "Epoch [4/20], Step [81/90], Loss: 1.0126\n",
      "Epoch [5/20], Step [1/90], Loss: 0.8616\n",
      "Epoch [5/20], Step [11/90], Loss: 0.8862\n",
      "Epoch [5/20], Step [21/90], Loss: 0.9109\n",
      "Epoch [5/20], Step [31/90], Loss: 0.8832\n",
      "Epoch [5/20], Step [41/90], Loss: 0.8438\n",
      "Epoch [5/20], Step [51/90], Loss: 1.0271\n",
      "Epoch [5/20], Step [61/90], Loss: 1.0480\n",
      "Epoch [5/20], Step [71/90], Loss: 0.9906\n",
      "Epoch [5/20], Step [81/90], Loss: 0.9426\n",
      "Epoch [6/20], Step [1/90], Loss: 0.9310\n",
      "Epoch [6/20], Step [11/90], Loss: 0.7273\n",
      "Epoch [6/20], Step [21/90], Loss: 0.7079\n",
      "Epoch [6/20], Step [31/90], Loss: 0.7308\n",
      "Epoch [6/20], Step [41/90], Loss: 0.9268\n",
      "Epoch [6/20], Step [51/90], Loss: 0.9034\n",
      "Epoch [6/20], Step [61/90], Loss: 0.9013\n",
      "Epoch [6/20], Step [71/90], Loss: 0.8684\n",
      "Epoch [6/20], Step [81/90], Loss: 0.8648\n",
      "Epoch [7/20], Step [1/90], Loss: 0.8282\n",
      "Epoch [7/20], Step [11/90], Loss: 0.7938\n",
      "Epoch [7/20], Step [21/90], Loss: 0.9434\n",
      "Epoch [7/20], Step [31/90], Loss: 0.8378\n",
      "Epoch [7/20], Step [41/90], Loss: 0.8440\n",
      "Epoch [7/20], Step [51/90], Loss: 0.8677\n",
      "Epoch [7/20], Step [61/90], Loss: 0.9701\n",
      "Epoch [7/20], Step [71/90], Loss: 1.1405\n",
      "Epoch [7/20], Step [81/90], Loss: 0.9180\n",
      "Epoch [8/20], Step [1/90], Loss: 0.7097\n",
      "Epoch [8/20], Step [11/90], Loss: 0.9118\n",
      "Epoch [8/20], Step [21/90], Loss: 0.9122\n",
      "Epoch [8/20], Step [31/90], Loss: 0.7482\n",
      "Epoch [8/20], Step [41/90], Loss: 0.8182\n",
      "Epoch [8/20], Step [51/90], Loss: 0.7396\n",
      "Epoch [8/20], Step [61/90], Loss: 0.8808\n",
      "Epoch [8/20], Step [71/90], Loss: 0.9739\n",
      "Epoch [8/20], Step [81/90], Loss: 0.9062\n",
      "Epoch [9/20], Step [1/90], Loss: 0.6270\n",
      "Epoch [9/20], Step [11/90], Loss: 0.8848\n",
      "Epoch [9/20], Step [21/90], Loss: 0.6872\n",
      "Epoch [9/20], Step [31/90], Loss: 0.9273\n",
      "Epoch [9/20], Step [41/90], Loss: 0.6663\n",
      "Epoch [9/20], Step [51/90], Loss: 0.7606\n",
      "Epoch [9/20], Step [61/90], Loss: 0.9897\n",
      "Epoch [9/20], Step [71/90], Loss: 1.0131\n",
      "Epoch [9/20], Step [81/90], Loss: 0.8218\n",
      "Epoch [10/20], Step [1/90], Loss: 0.7087\n",
      "Epoch [10/20], Step [11/90], Loss: 0.8444\n",
      "Epoch [10/20], Step [21/90], Loss: 0.7937\n",
      "Epoch [10/20], Step [31/90], Loss: 0.7127\n",
      "Epoch [10/20], Step [41/90], Loss: 0.7864\n",
      "Epoch [10/20], Step [51/90], Loss: 0.9770\n",
      "Epoch [10/20], Step [61/90], Loss: 0.7745\n",
      "Epoch [10/20], Step [71/90], Loss: 0.7597\n",
      "Epoch [10/20], Step [81/90], Loss: 0.7124\n",
      "Epoch [11/20], Step [1/90], Loss: 0.5890\n",
      "Epoch [11/20], Step [11/90], Loss: 0.5919\n",
      "Epoch [11/20], Step [21/90], Loss: 0.7847\n",
      "Epoch [11/20], Step [31/90], Loss: 0.7030\n",
      "Epoch [11/20], Step [41/90], Loss: 1.1054\n",
      "Epoch [11/20], Step [51/90], Loss: 0.7550\n",
      "Epoch [11/20], Step [61/90], Loss: 0.7171\n",
      "Epoch [11/20], Step [71/90], Loss: 0.9887\n",
      "Epoch [11/20], Step [81/90], Loss: 0.8747\n",
      "Epoch [12/20], Step [1/90], Loss: 0.6850\n",
      "Epoch [12/20], Step [11/90], Loss: 0.6241\n",
      "Epoch [12/20], Step [21/90], Loss: 0.5494\n",
      "Epoch [12/20], Step [31/90], Loss: 0.6513\n",
      "Epoch [12/20], Step [41/90], Loss: 0.7334\n",
      "Epoch [12/20], Step [51/90], Loss: 0.7702\n",
      "Epoch [12/20], Step [61/90], Loss: 0.8185\n",
      "Epoch [12/20], Step [71/90], Loss: 0.8223\n",
      "Epoch [12/20], Step [81/90], Loss: 0.7287\n",
      "Epoch [13/20], Step [1/90], Loss: 0.6631\n",
      "Epoch [13/20], Step [11/90], Loss: 0.6031\n",
      "Epoch [13/20], Step [21/90], Loss: 0.6330\n",
      "Epoch [13/20], Step [31/90], Loss: 0.7270\n",
      "Epoch [13/20], Step [41/90], Loss: 0.8281\n",
      "Epoch [13/20], Step [51/90], Loss: 0.6733\n",
      "Epoch [13/20], Step [61/90], Loss: 0.7929\n",
      "Epoch [13/20], Step [71/90], Loss: 0.7454\n",
      "Epoch [13/20], Step [81/90], Loss: 0.8811\n",
      "Epoch [14/20], Step [1/90], Loss: 0.6718\n",
      "Epoch [14/20], Step [11/90], Loss: 0.5193\n",
      "Epoch [14/20], Step [21/90], Loss: 0.8129\n",
      "Epoch [14/20], Step [31/90], Loss: 0.6451\n",
      "Epoch [14/20], Step [41/90], Loss: 0.7448\n",
      "Epoch [14/20], Step [51/90], Loss: 0.5933\n",
      "Epoch [14/20], Step [61/90], Loss: 0.6003\n",
      "Epoch [14/20], Step [71/90], Loss: 0.8032\n",
      "Epoch [14/20], Step [81/90], Loss: 0.8694\n",
      "Epoch [15/20], Step [1/90], Loss: 0.5571\n",
      "Epoch [15/20], Step [11/90], Loss: 0.6058\n",
      "Epoch [15/20], Step [21/90], Loss: 0.6431\n",
      "Epoch [15/20], Step [31/90], Loss: 0.7874\n",
      "Epoch [15/20], Step [41/90], Loss: 0.6982\n",
      "Epoch [15/20], Step [51/90], Loss: 0.7099\n",
      "Epoch [15/20], Step [61/90], Loss: 0.6712\n",
      "Epoch [15/20], Step [71/90], Loss: 0.7541\n",
      "Epoch [15/20], Step [81/90], Loss: 0.9266\n",
      "Epoch [16/20], Step [1/90], Loss: 0.6488\n",
      "Epoch [16/20], Step [11/90], Loss: 0.5800\n",
      "Epoch [16/20], Step [21/90], Loss: 0.6017\n",
      "Epoch [16/20], Step [31/90], Loss: 0.7267\n",
      "Epoch [16/20], Step [41/90], Loss: 0.7248\n",
      "Epoch [16/20], Step [51/90], Loss: 0.7368\n",
      "Epoch [16/20], Step [61/90], Loss: 0.6210\n",
      "Epoch [16/20], Step [71/90], Loss: 0.7707\n",
      "Epoch [16/20], Step [81/90], Loss: 0.7380\n",
      "Epoch [17/20], Step [1/90], Loss: 0.5541\n",
      "Epoch [17/20], Step [11/90], Loss: 0.5568\n",
      "Epoch [17/20], Step [21/90], Loss: 0.4068\n",
      "Epoch [17/20], Step [31/90], Loss: 0.6103\n",
      "Epoch [17/20], Step [41/90], Loss: 0.6756\n",
      "Epoch [17/20], Step [51/90], Loss: 0.7850\n",
      "Epoch [17/20], Step [61/90], Loss: 0.7053\n",
      "Epoch [17/20], Step [71/90], Loss: 0.9652\n",
      "Epoch [17/20], Step [81/90], Loss: 0.5229\n",
      "Epoch [18/20], Step [1/90], Loss: 0.6485\n",
      "Epoch [18/20], Step [11/90], Loss: 0.6724\n",
      "Epoch [18/20], Step [21/90], Loss: 0.5370\n",
      "Epoch [18/20], Step [31/90], Loss: 0.6255\n",
      "Epoch [18/20], Step [41/90], Loss: 0.6806\n",
      "Epoch [18/20], Step [51/90], Loss: 0.4878\n",
      "Epoch [18/20], Step [61/90], Loss: 0.7496\n",
      "Epoch [18/20], Step [71/90], Loss: 0.5767\n",
      "Epoch [18/20], Step [81/90], Loss: 0.5605\n",
      "Epoch [19/20], Step [1/90], Loss: 0.6152\n",
      "Epoch [19/20], Step [11/90], Loss: 0.4728\n",
      "Epoch [19/20], Step [21/90], Loss: 0.6540\n",
      "Epoch [19/20], Step [31/90], Loss: 0.6575\n",
      "Epoch [19/20], Step [41/90], Loss: 0.6066\n",
      "Epoch [19/20], Step [51/90], Loss: 0.6526\n",
      "Epoch [19/20], Step [61/90], Loss: 0.6878\n",
      "Epoch [19/20], Step [71/90], Loss: 0.7965\n",
      "Epoch [19/20], Step [81/90], Loss: 0.7499\n",
      "Epoch [20/20], Step [1/90], Loss: 0.6084\n",
      "Epoch [20/20], Step [11/90], Loss: 0.4923\n",
      "Epoch [20/20], Step [21/90], Loss: 0.5527\n",
      "Epoch [20/20], Step [31/90], Loss: 0.5154\n",
      "Epoch [20/20], Step [41/90], Loss: 0.6234\n",
      "Epoch [20/20], Step [51/90], Loss: 0.7246\n",
      "Epoch [20/20], Step [61/90], Loss: 0.6272\n",
      "Epoch [20/20], Step [71/90], Loss: 0.7245\n",
      "Epoch [20/20], Step [81/90], Loss: 0.7609\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T03:05:43.678631Z",
     "start_time": "2025-05-14T03:05:42.894824Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.eval()  # eval mode (batchnorm uses moving mean/variance instead of mini-batch mean/variance)\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in validation_dataloader:\n",
    "        images = images.to(compute_device)\n",
    "        labels = labels.to(compute_device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Test Accuracy of the model on the test images: {} %'.format(100 * correct / total))\n",
    "\n",
    "# Save the model checkpoint\n",
    "# torch.save(model.state_dict(), 'model.ckpt')"
   ],
   "id": "8e63274732ef50a2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the model on the test images: 39.385474860335194 %\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "b5f818d7a2ff305f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
