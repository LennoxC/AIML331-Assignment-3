{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## AIML331 Assignment 3 Continued\n",
    "### crowelenn, 300607096\n",
    "\n",
    "In the second part of this assignment, the focus is on using vision transformers for image classification.\n",
    "\n",
    "#### Setup"
   ],
   "id": "1cb87afcd79e6491"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T04:58:37.134130Z",
     "start_time": "2025-05-23T04:58:25.140108Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from dataset_wrapper import get_pet_datasets\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ],
   "id": "beef74d80bd8e6a4",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T04:59:16.184364Z",
     "start_time": "2025-05-23T04:58:39.548048Z"
    }
   },
   "cell_type": "code",
   "source": "train_dataset, val_dataset, test_dataset = get_pet_datasets(img_width=128, img_height=128,root_path='./data' )",
   "id": "69c26efc5717828d",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T04:59:16.198265Z",
     "start_time": "2025-05-23T04:59:16.190305Z"
    }
   },
   "cell_type": "code",
   "source": "print(f\"Loaded data, train = {len(train_dataset)}, test = {len(test_dataset)}\")",
   "id": "e63d4f5881564129",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data, train = 5719, test = 716\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T04:59:17.629605Z",
     "start_time": "2025-05-23T04:59:16.259644Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(torch.cuda.is_available())\n",
    "compute_device = torch.device('cuda:0')"
   ],
   "id": "61a9302705718c7b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T04:59:17.649705Z",
     "start_time": "2025-05-23T04:59:17.644023Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# load the datasets\n",
    "batch_size = 32\n",
    "training_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "validation_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "testing_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ],
   "id": "9a5cbc9c47eafd9e",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Building the vision transformer",
   "id": "6164ef95b4b1d545"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T04:59:17.675247Z",
     "start_time": "2025-05-23T04:59:17.665525Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "def get_embedding(positions, dim):\n",
    "    position = torch.arange(positions).unsqueeze(1)\n",
    "    div_term = torch.exp(torch.arange(0, dim, 2) * (-np.log(10000.0) / dim))\n",
    "\n",
    "    pe = torch.zeros(positions, dim)\n",
    "    pe[:, 0::2] = torch.sin(position * div_term)\n",
    "    pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "    return pe.unsqueeze(0)\n",
    "\n",
    "def image_to_patch(x, patch_size):\n",
    "    B, C, H, W = x.shape\n",
    "    x = x.reshape(B, C, H//patch_size, patch_size, W//patch_size, patch_size)\n",
    "    x = x.permute(0, 2, 4, 1, 3, 5)\n",
    "    x = x.flatten(1,2)\n",
    "    return x"
   ],
   "id": "762db6007b95286e",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T04:59:17.702255Z",
     "start_time": "2025-05-23T04:59:17.695258Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, embedding_dims, hidden_dims, count_heads, dropout=0.2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer_normalisation_1 = nn.LayerNorm(embedding_dims)\n",
    "        self.Attention = nn.MultiheadAttention(embedding_dims, num_heads=count_heads, dropout=dropout)\n",
    "        self.layer_normalisation_2 = nn.LayerNorm(embedding_dims)\n",
    "        self.Linear = nn.Sequential(\n",
    "            nn.Linear(embedding_dims, hidden_dims),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dims, embedding_dims),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_x):\n",
    "        input_norm = self.layer_normalisation_1(input_x)\n",
    "        attention_out, _ = self.Attention(input_norm, input_norm, input_norm)\n",
    "        input_x = input_x + attention_out\n",
    "\n",
    "        norm_out = self.layer_normalisation_2(input_x)\n",
    "        input_x = input_x + self.Linear(norm_out)\n",
    "\n",
    "        return input_x"
   ],
   "id": "1bdbc76cce2caece",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T04:59:17.747845Z",
     "start_time": "2025-05-23T04:59:17.739507Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class PetsViT(nn.Module):\n",
    "    def __init__(self, embedding_dims=60, hidden_dims=256, channels=3, count_heads=3, count_layers=3, classes=4, patch_size=16, input_dims=128, dropout=0.2, learnable_positional_embeddings=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.count_patches = (input_dims//patch_size)*(input_dims//patch_size)\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "        # neural net layers\n",
    "        self.input_layer = nn.Linear(channels*(patch_size*patch_size), embedding_dims)\n",
    "        self.transformer = nn.Sequential(*[AttentionBlock(embedding_dims, hidden_dims, count_heads, dropout=dropout) for _ in range(count_layers)])\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.LayerNorm(embedding_dims),\n",
    "            nn.Linear(embedding_dims, classes)\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, embedding_dims))\n",
    "        if learnable_positional_embeddings:\n",
    "            self.position_embeddings = nn.Parameter(torch.randn(1, 1 + self.count_patches, embedding_dims))\n",
    "        else:\n",
    "            self.register_buffer('position_embeddings', get_embedding(1 + self.count_patches, embedding_dims))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = image_to_patch(x, self.patch_size)\n",
    "        x = x.flatten(2,4)\n",
    "        B, T, _ = x.shape\n",
    "        x = self.input_layer(x)\n",
    "\n",
    "        cls_token = self.cls_token.repeat(B, 1, 1)\n",
    "        x = torch.cat([cls_token, x], dim=1)\n",
    "        x = x + self.position_embeddings[:,:T+1]\n",
    "\n",
    "        x = self.dropout(x)\n",
    "        x = x.transpose(0, 1)\n",
    "        x = self.transformer(x)\n",
    "\n",
    "        cls = x[0]\n",
    "        out = self.mlp(cls)\n",
    "        return out"
   ],
   "id": "650b68ae3566d6d1",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Copy the train & test functions from the previous question",
   "id": "bd9cbfd23c97876d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T04:59:17.769329Z",
     "start_time": "2025-05-23T04:59:17.759891Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def reset_weights(m):\n",
    "    if isinstance(m, (nn.Linear, nn.Conv2d)):\n",
    "        m.reset_parameters()\n",
    "\n",
    "def do_training(model, experiment_name, criterion, optimizer, num_epochs=20, patience=5):\n",
    "    writer = SummaryWriter('runs/'+experiment_name)\n",
    "\n",
    "    min_validation_loss = None\n",
    "    best_model_state = None # store the best model here. Re-instate this if early stopping is triggered\n",
    "    wait = 0\n",
    "\n",
    "    model.apply(reset_weights)\n",
    "\n",
    "    steps = len(training_dataloader)\n",
    "    for epoch in range(num_epochs): # epoch iteration loop\n",
    "        model.train()\n",
    "        train_loss_epoch_total = 0\n",
    "        batches_count = 0\n",
    "\n",
    "        for i, (images, labels) in enumerate(training_dataloader):\n",
    "\n",
    "            if i == 0:\n",
    "                writer.add_graph(model, images.to(compute_device))\n",
    "            images = images.to(compute_device)\n",
    "            labels = labels.to(compute_device)\n",
    "\n",
    "            # forwards\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # backpropogation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss_epoch_total += loss.item()\n",
    "            batches_count += 1\n",
    "\n",
    "        train_loss = train_loss_epoch_total / batches_count\n",
    "        writer.add_scalar('Loss/train', train_loss, epoch+1)\n",
    "\n",
    "        # validation accuracy\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            val_loss_epoch_total = 0\n",
    "            val_batches_count = 0\n",
    "\n",
    "            for images, labels in validation_dataloader:\n",
    "                images = images.to(compute_device)\n",
    "                labels = labels.to(compute_device)\n",
    "                outputs = model(images)\n",
    "                val_loss = criterion(outputs, labels)\n",
    "\n",
    "                val_loss_epoch_total += val_loss.item()\n",
    "                val_batches_count += 1\n",
    "\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        val_acc = 100 * correct / total\n",
    "        val_loss = val_loss_epoch_total / val_batches_count\n",
    "\n",
    "        writer.add_scalar('Accuracy/validation', val_acc, epoch)\n",
    "        writer.add_scalar('Loss/validation', val_loss, epoch)\n",
    "\n",
    "        if min_validation_loss is None or val_loss < min_validation_loss:\n",
    "            min_validation_loss = val_loss\n",
    "            best_model_state = model.state_dict() # save the best weights\n",
    "            wait = 0\n",
    "        else:\n",
    "            wait += 1\n",
    "\n",
    "        if wait >= patience:\n",
    "            break # exit early if there has been no improvement in validation loss\n",
    "\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "        print(\"Best model weights restored.\")\n",
    "\n",
    "    writer.close()\n",
    "\n",
    "    return model"
   ],
   "id": "97d0fb4e115c520b",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T04:59:17.785984Z",
     "start_time": "2025-05-23T04:59:17.780684Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def do_testing(model, dataloader):\n",
    "    model.eval()  # eval mode (batchnorm uses moving mean/variance instead of mini-batch mean/variance)\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in dataloader:\n",
    "            images = images.to(compute_device)\n",
    "            labels = labels.to(compute_device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            accuracy = 100 * correct / total\n",
    "\n",
    "        return accuracy, 'Accuracy of the model on the provided images: {} %'.format(accuracy)"
   ],
   "id": "4946e14b8d6b2a72",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Define a model for each test",
   "id": "bde7a9f6eb91a0a6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T04:59:21.329729Z",
     "start_time": "2025-05-23T04:59:20.937665Z"
    }
   },
   "cell_type": "code",
   "source": [
    "baseline_model = PetsViT().to(compute_device)\n",
    "\n",
    "# vary the attention heads\n",
    "model_attn_4 = PetsViT(count_heads=4).to(compute_device)\n",
    "model_attn_5 = PetsViT(count_heads=5).to(compute_device)\n",
    "model_attn_6 = PetsViT(count_heads=6).to(compute_device)\n",
    "\n",
    "# vary the layers\n",
    "model_layers_4 = PetsViT(count_layers=4).to(compute_device)\n",
    "model_layers_5 = PetsViT(count_layers=5).to(compute_device)\n",
    "model_layers_6 = PetsViT(count_layers=6).to(compute_device)\n",
    "\n",
    "# vary the patch sizes\n",
    "model_patches_8 = PetsViT(patch_size=8).to(compute_device)\n",
    "model_patches_4 = PetsViT(patch_size=4).to(compute_device)\n",
    "\n",
    "# test with/without positional embeddings\n",
    "model_positionalembeddings = PetsViT(learnable_positional_embeddings=True).to(compute_device)"
   ],
   "id": "6a71e2d4c212bb93",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T05:20:10.253396Z",
     "start_time": "2025-05-23T05:20:10.233303Z"
    }
   },
   "cell_type": "code",
   "source": [
    "models_params = [baseline_model, model_attn_4, model_attn_5, model_attn_6, model_layers_4, model_layers_5, model_layers_6, model_patches_8, model_patches_4]\n",
    "model_param_counts = []\n",
    "\n",
    "for i in range(len(models_params)):\n",
    "    model_param_counts.append(sum(param.numel() for param in models_params[i].parameters()))"
   ],
   "id": "e86b9887ff6b147e",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T05:20:12.902429Z",
     "start_time": "2025-05-23T05:20:12.415174Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(model_param_counts)"
   ],
   "id": "34eb7ed461158ec1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2., 0., 4., 0., 1., 0., 0., 1., 0., 1.]),\n",
       " array([141112. , 159206.8, 177301.6, 195396.4, 213491.2, 231586. ,\n",
       "        249680.8, 267775.6, 285870.4, 303965.2, 322060. ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Run the experiments",
   "id": "ee5e5eb595bba65"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T23:57:58.266222Z",
     "start_time": "2025-05-20T23:57:58.260735Z"
    }
   },
   "cell_type": "code",
   "source": [
    "weights = [600/7149, 1771/7149, 2590/7149, 2188/7149]\n",
    "class_weights = torch.FloatTensor(weights).cuda()\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "learning_rate = 0.001"
   ],
   "id": "750a1f1c3035ad22",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T00:03:13.199559Z",
     "start_time": "2025-05-20T23:57:58.656029Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# train the baseline model\n",
    "optimizer = torch.optim.Adam(baseline_model.parameters(), lr=learning_rate)\n",
    "\n",
    "baseline_trained = do_training(baseline_model, 'vision_transformer_baseline', criterion, optimizer, num_epochs=100, patience=20)"
   ],
   "id": "db437a0fc2a5eb6d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model weights restored.\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T00:03:13.449029Z",
     "start_time": "2025-05-21T00:03:13.257107Z"
    }
   },
   "cell_type": "code",
   "source": [
    "acc, acc_string = do_testing(baseline_trained, testing_dataloader)\n",
    "print(acc_string)"
   ],
   "id": "d569ade3dc427477",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the provided images: 46.927374301675975 %\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 4 Attention Heads",
   "id": "336ea2de398c81d8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T00:07:17.043730Z",
     "start_time": "2025-05-21T00:03:13.475192Z"
    }
   },
   "cell_type": "code",
   "source": [
    "optim = torch.optim.Adam(model_attn_4.parameters(), lr=learning_rate)\n",
    "attn_4_trained = do_training(model_attn_4, 'vision_transformer_4_attn', criterion, optim, num_epochs=100, patience=20)\n",
    "acc, acc_string = do_testing(attn_4_trained, testing_dataloader)\n",
    "print(acc_string)"
   ],
   "id": "fd4329f1e527fb3c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model weights restored.\n",
      "Accuracy of the model on the provided images: 45.94972067039106 %\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 5 Attention Heads",
   "id": "b76b4b05d9de080e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T00:12:23.471435Z",
     "start_time": "2025-05-21T00:07:17.133601Z"
    }
   },
   "cell_type": "code",
   "source": [
    "optim = torch.optim.Adam(model_attn_5.parameters(), lr=learning_rate)\n",
    "attn_5_trained = do_training(model_attn_5, 'vision_transformer_5_attn', criterion, optim, num_epochs=100, patience=20)\n",
    "acc, acc_string = do_testing(attn_5_trained, testing_dataloader)\n",
    "print(acc_string)"
   ],
   "id": "97fb0db5c06347ce",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model weights restored.\n",
      "Accuracy of the model on the provided images: 47.20670391061452 %\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 6 Attention Heads",
   "id": "8b0a0bd260de1181"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T00:16:36.607209Z",
     "start_time": "2025-05-21T00:12:23.597735Z"
    }
   },
   "cell_type": "code",
   "source": [
    "optim = torch.optim.Adam(model_attn_6.parameters(), lr=learning_rate)\n",
    "attn_6_trained = do_training(model_attn_6, 'vision_transformer_6_attn', criterion, optim, num_epochs=100, patience=20)\n",
    "acc, acc_string = do_testing(attn_6_trained, testing_dataloader)\n",
    "print(acc_string)"
   ],
   "id": "7d95dc8c1896538e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model weights restored.\n",
      "Accuracy of the model on the provided images: 44.41340782122905 %\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 4 Transformer Layers",
   "id": "90c118a5fb5d3d98"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T00:22:21.465906Z",
     "start_time": "2025-05-21T00:16:36.697635Z"
    }
   },
   "cell_type": "code",
   "source": [
    "optim = torch.optim.Adam(model_layers_4.parameters(), lr=learning_rate)\n",
    "model_layers_4_trained = do_training(model_layers_4, 'vision_transformer_4_layers', criterion, optim, num_epochs=100, patience=20)\n",
    "acc, acc_string = do_testing(model_layers_4_trained, testing_dataloader)\n",
    "print(acc_string)"
   ],
   "id": "c2356f8f1390e40e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model weights restored.\n",
      "Accuracy of the model on the provided images: 46.229050279329606 %\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 5 Transformer Layers",
   "id": "30a284c987a52e47"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T00:28:58.374849Z",
     "start_time": "2025-05-21T00:22:21.550502Z"
    }
   },
   "cell_type": "code",
   "source": [
    "optim = torch.optim.Adam(model_layers_5.parameters(), lr=learning_rate)\n",
    "model_layers_5_trained = do_training(model_layers_5, 'vision_transformer_5_layers', criterion, optim, num_epochs=100, patience=20)\n",
    "acc, acc_string = do_testing(model_layers_5_trained, testing_dataloader)\n",
    "print(acc_string)"
   ],
   "id": "8d7c852307eaf071",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model weights restored.\n",
      "Accuracy of the model on the provided images: 49.16201117318436 %\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 6 Transformer Layers",
   "id": "3f871bc7f7aebac9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T00:37:23.190512Z",
     "start_time": "2025-05-21T00:28:58.451907Z"
    }
   },
   "cell_type": "code",
   "source": [
    "optim = torch.optim.Adam(model_layers_6.parameters(), lr=learning_rate)\n",
    "model_layers_6_trained = do_training(model_layers_6, 'vision_transformer_6_layers', criterion, optim, num_epochs=100, patience=20)\n",
    "acc, acc_string = do_testing(model_layers_6_trained, testing_dataloader)\n",
    "print(acc_string)"
   ],
   "id": "bf1ce63ad1990bf8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model weights restored.\n",
      "Accuracy of the model on the provided images: 45.53072625698324 %\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Learnable Positional Embeddings",
   "id": "4cbb83af4b7c2914"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T00:42:28.516223Z",
     "start_time": "2025-05-21T00:37:51.530674Z"
    }
   },
   "cell_type": "code",
   "source": [
    "optim = torch.optim.Adam(model_positionalembeddings.parameters(), lr=learning_rate)\n",
    "model_posembeddings_trained = do_training(model_positionalembeddings, 'vision_transformer_posembeddings', criterion, optim, num_epochs=100, patience=20)\n",
    "acc, acc_string = do_testing(model_posembeddings_trained, testing_dataloader)\n",
    "print(acc_string)"
   ],
   "id": "489b5299ca63dc9d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model weights restored.\n",
      "Accuracy of the model on the provided images: 47.625698324022345 %\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Patch Size of 8",
   "id": "6a5f0a595675713e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T07:18:10.103538Z",
     "start_time": "2025-05-23T06:56:24.521229Z"
    }
   },
   "cell_type": "code",
   "source": [
    "optim = torch.optim.Adam(model_patches_8.parameters(), lr=learning_rate)\n",
    "model_patches_8_trained = do_training(model_patches_8, 'vision_transformer_8_patchsize', criterion, optim, num_epochs=100, patience=20)\n",
    "acc, acc_string = do_testing(model_patches_8_trained, testing_dataloader)\n",
    "print(acc_string)"
   ],
   "id": "58fde7fa96c2e462",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model weights restored.\n",
      "Accuracy of the model on the provided images: 53.072625698324025 %\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Patch Size of 4",
   "id": "391817bba2cbe87b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T09:25:27.241510Z",
     "start_time": "2025-05-23T07:18:10.228784Z"
    }
   },
   "cell_type": "code",
   "source": [
    "optim = torch.optim.Adam(model_patches_4.parameters(), lr=0.005)\n",
    "model_patches_4_trained = do_training(model_patches_4, 'vision_transformer_4_patchsize', criterion, optim, num_epochs=60, patience=7)\n",
    "acc, acc_string = do_testing(model_patches_4_trained, testing_dataloader)\n",
    "print(acc_string)"
   ],
   "id": "50e5383405897906",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model weights restored.\n",
      "Accuracy of the model on the provided images: 39.944134078212294 %\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Testing the inference time\n",
    "I am training a new baseline model, as I have already written the bulk of the report, and don't want to have to update the baseline model accuracy & figures throughout."
   ],
   "id": "66b2b2a3cc569cd7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T05:32:00.173913Z",
     "start_time": "2025-05-23T05:28:19.796026Z"
    }
   },
   "cell_type": "code",
   "source": [
    "baseline_model_inference = PetsViT().to(compute_device)\n",
    "\n",
    "weights = [600 / 7149, 1771 / 7149, 2590 / 7149, 2188 / 7149]\n",
    "class_weights = torch.FloatTensor(weights).cuda()\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "learning_rate = 0.001\n",
    "# train the baseline model\n",
    "optimizer = torch.optim.Adam(baseline_model_inference.parameters(), lr=learning_rate)\n",
    "\n",
    "baseline_trained = do_training(baseline_model_inference, 'vision_transformer_baseline_inference', criterion, optimizer, num_epochs=100, patience=20)"
   ],
   "id": "1defbecb0abe30be",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model weights restored.\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T05:32:09.849855Z",
     "start_time": "2025-05-23T05:32:09.461228Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import time\n",
    "\n",
    "baseline_trained.eval()\n",
    "\n",
    "total_time = 0.0\n",
    "total_images = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, _ in testing_dataloader:\n",
    "        inputs = inputs.to(compute_device)\n",
    "        batch_size = inputs.size(0)\n",
    "\n",
    "        start_time = time.time()\n",
    "        _ = baseline_trained(inputs)\n",
    "        end_time = time.time()\n",
    "\n",
    "        total_time += end_time - start_time\n",
    "        total_images += batch_size\n",
    "\n",
    "average_inference = total_time / total_images\n",
    "print(f\"Average image inference time: {average_inference}\")"
   ],
   "id": "9c551c5dffa0230c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average image inference time: 7.797452990569216e-05\n"
     ]
    }
   ],
   "execution_count": 20
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
