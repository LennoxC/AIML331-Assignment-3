{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## AIML331 Assignment 3 Continued\n",
    "### crowelenn, 300607096\n",
    "\n",
    "In the second part of this assignment, the focus is on using vision transformers for image classification.\n",
    "\n",
    "#### Setup"
   ],
   "id": "1cb87afcd79e6491"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T23:57:19.406589Z",
     "start_time": "2025-05-20T23:56:52.067184Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from dataset_wrapper import get_pet_datasets\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ],
   "id": "beef74d80bd8e6a4",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T23:57:55.721931Z",
     "start_time": "2025-05-20T23:57:19.410594Z"
    }
   },
   "cell_type": "code",
   "source": "train_dataset, val_dataset, test_dataset = get_pet_datasets(img_width=128, img_height=128,root_path='./data' )",
   "id": "69c26efc5717828d",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T23:57:56.559903Z",
     "start_time": "2025-05-20T23:57:56.554025Z"
    }
   },
   "cell_type": "code",
   "source": "print(f\"Loaded data, train = {len(train_dataset)}, test = {len(test_dataset)}\")",
   "id": "e63d4f5881564129",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data, train = 5719, test = 716\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T23:57:57.911955Z",
     "start_time": "2025-05-20T23:57:56.590190Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(torch.cuda.is_available())\n",
    "compute_device = torch.device('cuda:0')"
   ],
   "id": "61a9302705718c7b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T23:57:57.920656Z",
     "start_time": "2025-05-20T23:57:57.916356Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# load the datasets\n",
    "batch_size = 32\n",
    "training_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "validation_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "testing_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ],
   "id": "9a5cbc9c47eafd9e",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Building the vision transformer",
   "id": "6164ef95b4b1d545"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T23:57:57.961647Z",
     "start_time": "2025-05-20T23:57:57.956541Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "def get_embedding(positions, dim):\n",
    "    position = torch.arange(positions).unsqueeze(1)\n",
    "    div_term = torch.exp(torch.arange(0, dim, 2) * (-np.log(10000.0) / dim))\n",
    "\n",
    "    pe = torch.zeros(positions, dim)\n",
    "    pe[:, 0::2] = torch.sin(position * div_term)\n",
    "    pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "    return pe.unsqueeze(0)\n",
    "\n",
    "def image_to_patch(x, patch_size):\n",
    "    B, C, H, W = x.shape\n",
    "    x = x.reshape(B, C, H//patch_size, patch_size, W//patch_size, patch_size)\n",
    "    x = x.permute(0, 2, 4, 1, 3, 5)\n",
    "    x = x.flatten(1,2)\n",
    "    return x"
   ],
   "id": "762db6007b95286e",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T23:57:57.970958Z",
     "start_time": "2025-05-20T23:57:57.964656Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, embedding_dims, hidden_dims, count_heads, dropout=0.2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer_normalisation_1 = nn.LayerNorm(embedding_dims)\n",
    "        self.Attention = nn.MultiheadAttention(embedding_dims, num_heads=count_heads, dropout=dropout)\n",
    "        self.layer_normalisation_2 = nn.LayerNorm(embedding_dims)\n",
    "        self.Linear = nn.Sequential(\n",
    "            nn.Linear(embedding_dims, hidden_dims),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dims, embedding_dims),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_x):\n",
    "        input_norm = self.layer_normalisation_1(input_x)\n",
    "        attention_out, _ = self.Attention(input_norm, input_norm, input_norm)\n",
    "        input_x = input_x + attention_out\n",
    "\n",
    "        norm_out = self.layer_normalisation_2(input_x)\n",
    "        input_x = input_x + self.Linear(norm_out)\n",
    "\n",
    "        return input_x"
   ],
   "id": "1bdbc76cce2caece",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T23:57:57.988947Z",
     "start_time": "2025-05-20T23:57:57.979948Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class PetsViT(nn.Module):\n",
    "    def __init__(self, embedding_dims=60, hidden_dims=256, channels=3, count_heads=3, count_layers=3, classes=4, patch_size=16, input_dims=128, dropout=0.2, learnable_positional_embeddings=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.count_patches = (input_dims//patch_size)*(input_dims//patch_size)\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "        # neural net layers\n",
    "        self.input_layer = nn.Linear(channels*(patch_size*patch_size), embedding_dims)\n",
    "        self.transformer = nn.Sequential(*[AttentionBlock(embedding_dims, hidden_dims, count_heads, dropout=dropout) for _ in range(count_layers)])\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.LayerNorm(embedding_dims),\n",
    "            nn.Linear(embedding_dims, classes)\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, embedding_dims))\n",
    "        if learnable_positional_embeddings:\n",
    "            # or just False?\n",
    "            self.position_embeddings = nn.Parameter(torch.randn(1, 1 + self.count_patches, embedding_dims))\n",
    "        else:\n",
    "            self.register_buffer('position_embeddings', get_embedding(1 + self.count_patches, embedding_dims))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = image_to_patch(x, self.patch_size)\n",
    "        x = x.flatten(2,4)\n",
    "        B, T, _ = x.shape\n",
    "        x = self.input_layer(x)\n",
    "\n",
    "        cls_token = self.cls_token.repeat(B, 1, 1)\n",
    "        x = torch.cat([cls_token, x], dim=1)\n",
    "        x = x + self.position_embeddings[:,:T+1]\n",
    "\n",
    "        x = self.dropout(x)\n",
    "        x = x.transpose(0, 1)\n",
    "        x = self.transformer(x)\n",
    "\n",
    "        cls = x[0]\n",
    "        out = self.mlp(cls)\n",
    "        return out"
   ],
   "id": "650b68ae3566d6d1",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Copy the train & test functions from the previous question",
   "id": "bd9cbfd23c97876d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T23:57:58.006379Z",
     "start_time": "2025-05-20T23:57:57.997389Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def reset_weights(m):\n",
    "    if isinstance(m, (nn.Linear, nn.Conv2d)):\n",
    "        m.reset_parameters()\n",
    "\n",
    "def do_training(model, experiment_name, criterion, optimizer, num_epochs=20, patience=5):\n",
    "    writer = SummaryWriter('runs/'+experiment_name)\n",
    "\n",
    "    min_validation_loss = None\n",
    "    best_model_state = None # store the best model here. Re-instate this if early stopping is triggered\n",
    "    wait = 0\n",
    "\n",
    "    model.apply(reset_weights)\n",
    "\n",
    "    steps = len(training_dataloader)\n",
    "    for epoch in range(num_epochs): # epoch iteration loop\n",
    "        model.train()\n",
    "        train_loss_epoch_total = 0\n",
    "        batches_count = 0\n",
    "\n",
    "        for i, (images, labels) in enumerate(training_dataloader):\n",
    "\n",
    "            if i == 0:\n",
    "                writer.add_graph(model, images.to(compute_device))\n",
    "            images = images.to(compute_device)\n",
    "            labels = labels.to(compute_device)\n",
    "\n",
    "            # forwards\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # backpropogation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss_epoch_total += loss.item()\n",
    "            batches_count += 1\n",
    "\n",
    "        train_loss = train_loss_epoch_total / batches_count\n",
    "        writer.add_scalar('Loss/train', train_loss, epoch+1)\n",
    "\n",
    "        # validation accuracy\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            val_loss_epoch_total = 0\n",
    "            val_batches_count = 0\n",
    "\n",
    "            for images, labels in validation_dataloader:\n",
    "                images = images.to(compute_device)\n",
    "                labels = labels.to(compute_device)\n",
    "                outputs = model(images)\n",
    "                val_loss = criterion(outputs, labels)\n",
    "\n",
    "                val_loss_epoch_total += val_loss.item()\n",
    "                val_batches_count += 1\n",
    "\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        val_acc = 100 * correct / total\n",
    "        val_loss = val_loss_epoch_total / val_batches_count\n",
    "\n",
    "        writer.add_scalar('Accuracy/validation', val_acc, epoch)\n",
    "        writer.add_scalar('Loss/validation', val_loss, epoch)\n",
    "\n",
    "        if min_validation_loss is None or val_loss < min_validation_loss:\n",
    "            min_validation_loss = val_loss\n",
    "            best_model_state = model.state_dict() # save the best weights\n",
    "            wait = 0\n",
    "        else:\n",
    "            wait += 1\n",
    "\n",
    "        if wait >= patience:\n",
    "            break # exit early if there has been no improvement in validation loss\n",
    "\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "        print(\"Best model weights restored.\")\n",
    "\n",
    "    writer.close()\n",
    "\n",
    "    return model"
   ],
   "id": "97d0fb4e115c520b",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T23:57:58.020345Z",
     "start_time": "2025-05-20T23:57:58.014774Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def do_testing(model, dataloader):\n",
    "    model.eval()  # eval mode (batchnorm uses moving mean/variance instead of mini-batch mean/variance)\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in dataloader:\n",
    "            images = images.to(compute_device)\n",
    "            labels = labels.to(compute_device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            accuracy = 100 * correct / total\n",
    "\n",
    "        return accuracy, 'Accuracy of the model on the provided images: {} %'.format(accuracy)"
   ],
   "id": "4946e14b8d6b2a72",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Define a model for each test",
   "id": "bde7a9f6eb91a0a6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T23:57:58.249989Z",
     "start_time": "2025-05-20T23:57:58.028629Z"
    }
   },
   "cell_type": "code",
   "source": [
    "baseline_model = PetsViT().to(compute_device)\n",
    "\n",
    "# vary the attention heads\n",
    "model_attn_4 = PetsViT(count_heads=4).to(compute_device)\n",
    "model_attn_5 = PetsViT(count_heads=5).to(compute_device)\n",
    "model_attn_6 = PetsViT(count_heads=6).to(compute_device)\n",
    "\n",
    "# vary the layers\n",
    "model_layers_4 = PetsViT(count_layers=4).to(compute_device)\n",
    "model_layers_5 = PetsViT(count_layers=5).to(compute_device)\n",
    "model_layers_6 = PetsViT(count_layers=6).to(compute_device)\n",
    "\n",
    "# vary the patch sizes\n",
    "model_patches_8 = PetsViT(patch_size=8).to(compute_device)\n",
    "model_patches_4 = PetsViT(patch_size=4).to(compute_device)\n",
    "\n",
    "# test with/without positional embeddings\n",
    "model_positionalembeddings = PetsViT(learnable_positional_embeddings=True).to(compute_device)"
   ],
   "id": "6a71e2d4c212bb93",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Run the experiments",
   "id": "ee5e5eb595bba65"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T23:57:58.266222Z",
     "start_time": "2025-05-20T23:57:58.260735Z"
    }
   },
   "cell_type": "code",
   "source": [
    "weights = [600/7149, 1771/7149, 2590/7149, 2188/7149]\n",
    "class_weights = torch.FloatTensor(weights).cuda()\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "learning_rate = 0.001"
   ],
   "id": "750a1f1c3035ad22",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T00:03:13.199559Z",
     "start_time": "2025-05-20T23:57:58.656029Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# train the baseline model\n",
    "optimizer = torch.optim.Adam(baseline_model.parameters(), lr=learning_rate)\n",
    "\n",
    "baseline_trained = do_training(baseline_model, 'vision_transformer_baseline', criterion, optimizer, num_epochs=100, patience=20)"
   ],
   "id": "db437a0fc2a5eb6d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model weights restored.\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T00:03:13.449029Z",
     "start_time": "2025-05-21T00:03:13.257107Z"
    }
   },
   "cell_type": "code",
   "source": [
    "acc, acc_string = do_testing(baseline_trained, testing_dataloader)\n",
    "print(acc_string)"
   ],
   "id": "d569ade3dc427477",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the provided images: 46.927374301675975 %\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 4 Attention Heads",
   "id": "336ea2de398c81d8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T00:07:17.043730Z",
     "start_time": "2025-05-21T00:03:13.475192Z"
    }
   },
   "cell_type": "code",
   "source": [
    "optim = torch.optim.Adam(model_attn_4.parameters(), lr=learning_rate)\n",
    "attn_4_trained = do_training(model_attn_4, 'vision_transformer_4_attn', criterion, optim, num_epochs=100, patience=20)\n",
    "acc, acc_string = do_testing(attn_4_trained, testing_dataloader)\n",
    "print(acc_string)"
   ],
   "id": "fd4329f1e527fb3c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model weights restored.\n",
      "Accuracy of the model on the provided images: 45.94972067039106 %\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 5 Attention Heads",
   "id": "b76b4b05d9de080e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T00:12:23.471435Z",
     "start_time": "2025-05-21T00:07:17.133601Z"
    }
   },
   "cell_type": "code",
   "source": [
    "optim = torch.optim.Adam(model_attn_5.parameters(), lr=learning_rate)\n",
    "attn_5_trained = do_training(model_attn_5, 'vision_transformer_5_attn', criterion, optim, num_epochs=100, patience=20)\n",
    "acc, acc_string = do_testing(attn_5_trained, testing_dataloader)\n",
    "print(acc_string)"
   ],
   "id": "97fb0db5c06347ce",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model weights restored.\n",
      "Accuracy of the model on the provided images: 47.20670391061452 %\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 6 Attention Heads",
   "id": "8b0a0bd260de1181"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T00:16:36.607209Z",
     "start_time": "2025-05-21T00:12:23.597735Z"
    }
   },
   "cell_type": "code",
   "source": [
    "optim = torch.optim.Adam(model_attn_6.parameters(), lr=learning_rate)\n",
    "attn_6_trained = do_training(model_attn_6, 'vision_transformer_6_attn', criterion, optim, num_epochs=100, patience=20)\n",
    "acc, acc_string = do_testing(attn_6_trained, testing_dataloader)\n",
    "print(acc_string)"
   ],
   "id": "7d95dc8c1896538e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model weights restored.\n",
      "Accuracy of the model on the provided images: 44.41340782122905 %\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 4 Transformer Layers",
   "id": "90c118a5fb5d3d98"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T00:22:21.465906Z",
     "start_time": "2025-05-21T00:16:36.697635Z"
    }
   },
   "cell_type": "code",
   "source": [
    "optim = torch.optim.Adam(model_layers_4.parameters(), lr=learning_rate)\n",
    "model_layers_4_trained = do_training(model_layers_4, 'vision_transformer_4_layers', criterion, optim, num_epochs=100, patience=20)\n",
    "acc, acc_string = do_testing(model_layers_4_trained, testing_dataloader)\n",
    "print(acc_string)"
   ],
   "id": "c2356f8f1390e40e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model weights restored.\n",
      "Accuracy of the model on the provided images: 46.229050279329606 %\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 5 Transformer Layers",
   "id": "30a284c987a52e47"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T00:28:58.374849Z",
     "start_time": "2025-05-21T00:22:21.550502Z"
    }
   },
   "cell_type": "code",
   "source": [
    "optim = torch.optim.Adam(model_layers_5.parameters(), lr=learning_rate)\n",
    "model_layers_5_trained = do_training(model_layers_5, 'vision_transformer_5_layers', criterion, optim, num_epochs=100, patience=20)\n",
    "acc, acc_string = do_testing(model_layers_5_trained, testing_dataloader)\n",
    "print(acc_string)"
   ],
   "id": "8d7c852307eaf071",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model weights restored.\n",
      "Accuracy of the model on the provided images: 49.16201117318436 %\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 6 Transformer Layers",
   "id": "3f871bc7f7aebac9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T00:37:23.190512Z",
     "start_time": "2025-05-21T00:28:58.451907Z"
    }
   },
   "cell_type": "code",
   "source": [
    "optim = torch.optim.Adam(model_layers_6.parameters(), lr=learning_rate)\n",
    "model_layers_6_trained = do_training(model_layers_6, 'vision_transformer_6_layers', criterion, optim, num_epochs=100, patience=20)\n",
    "acc, acc_string = do_testing(model_layers_6_trained, testing_dataloader)\n",
    "print(acc_string)"
   ],
   "id": "bf1ce63ad1990bf8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model weights restored.\n",
      "Accuracy of the model on the provided images: 45.53072625698324 %\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Learnable Positional Embeddings",
   "id": "4cbb83af4b7c2914"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T00:42:28.516223Z",
     "start_time": "2025-05-21T00:37:51.530674Z"
    }
   },
   "cell_type": "code",
   "source": [
    "optim = torch.optim.Adam(model_positionalembeddings.parameters(), lr=learning_rate)\n",
    "model_posembeddings_trained = do_training(model_positionalembeddings, 'vision_transformer_posembeddings', criterion, optim, num_epochs=100, patience=20)\n",
    "acc, acc_string = do_testing(model_posembeddings_trained, testing_dataloader)\n",
    "print(acc_string)"
   ],
   "id": "489b5299ca63dc9d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model weights restored.\n",
      "Accuracy of the model on the provided images: 47.625698324022345 %\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Patch Size of 8",
   "id": "6a5f0a595675713e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "optim = torch.optim.Adam(model_patches_8.parameters(), lr=learning_rate)\n",
    "model_patches_8_trained = do_training(model_patches_8, 'vision_transformer_8_patchsize', criterion, optim, num_epochs=100, patience=20)\n",
    "acc, acc_string = do_testing(model_patches_8_trained, testing_dataloader)\n",
    "print(acc_string)"
   ],
   "id": "58fde7fa96c2e462"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Patch Size of 4",
   "id": "391817bba2cbe87b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "optim = torch.optim.Adam(model_patches_4.parameters(), lr=0.01)\n",
    "model_patches_4_trained = do_training(model_patches_4, 'vision_transformer_4_patchsize', criterion, optim, num_epochs=30, patience=10)\n",
    "acc, acc_string = do_testing(model_patches_4_trained, testing_dataloader)\n",
    "print(acc_string)"
   ],
   "id": "50e5383405897906"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
